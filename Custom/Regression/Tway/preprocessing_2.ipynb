{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a3cce77-bc69-49de-b06f-11495b359ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import awswrangler as wr\n",
    "import os \n",
    "bucket = 'poc-2209-twayairport-dp'\n",
    "\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa94c0b2-ba98-4404-96b6-e85510fa0988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key = 'train_data/coupon/pps_data/2018/08/coupon.parquet'\n",
    "\n",
    "os.environ['snsARN'] = 'arn:aws:sns:ap-northeast-2:279545419827:tway_lambda_report'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ab12f9-78b6-4117-9d4d-5b56e7d2aecf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.484375"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "52936704/1048576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166f1e9a-4908-4868-aacd-409d95c9ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "186507264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dbad8da-7553-42f9-a857-290661510b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9944.77734375"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10427854848/1048576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a8be2cd-b0bf-4aa4-adf3-7f1686e03f01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pmem(rss=10427854848, vms=13018349568, shared=74444800, text=2031616, lib=0, data=11508637696, dirty=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psutil\n",
    "p = psutil.Process()\n",
    "p.memory_info()\n",
    "# 1,048,576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e78a8850-766c-473c-979a-3b8e9db8a9ee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data/coupon/pps_data/2018/08/coupon.parquet\n",
      "2017-08-01 00:00:00 2018-08-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import awswrangler as wr\n",
    "import datetime \n",
    "import module_v1\n",
    "from contextlib import suppress\n",
    "print(key)\n",
    "\n",
    "## Preprocessing 2\n",
    "\n",
    "## Load coupon & booking Data\n",
    "a, b, c, year, month, filename = key.split('/')\n",
    "end_date = datetime.datetime.strptime(f'{year}-{int(month) + 1}','%Y-%m') - datetime.timedelta(days=1)\n",
    "start_date = end_date - datetime.timedelta(days=365)\n",
    "date_range = pd.date_range(start_date, end_date, freq= 'M')\n",
    "\n",
    "coupon_list = []\n",
    "booking_list = []\n",
    "\n",
    "for idx, date in enumerate(date_range):\n",
    "    if len(str(date.month)) == 1 :\n",
    "        temp_month = '0'+str(date.month)\n",
    "    else:\n",
    "        temp_month = date.month        \n",
    "    with suppress(Exception): coupon_list.append(wr.s3.read_parquet(f\"s3://{bucket}/train_data/coupon/pps_data/{date.year}/{temp_month}/coupon_df.parquet\"))\n",
    "    with suppress(Exception): booking_list.append(wr.s3.read_parquet(f\"s3://{bucket}/train_data/booking/pps_data/{date.year}/{temp_month}/booking_df.parquet\"))\n",
    "    \n",
    "coupon_df = pd.concat(coupon_list)\n",
    "booking_df = pd.concat(booking_list)\n",
    "    \n",
    "## Merge coupon_df & booking_df\n",
    "orig_df = pd.merge(booking_df, coupon_df, on=list(set(list(booking_df)) & set(list(coupon_df))), how = 'right')\n",
    "orig_df.dropna(axis = 0, inplace = True)\n",
    "\n",
    "## Load flight Data\n",
    "print(orig_df['flight_departure_date'].min(),orig_df['flight_departure_date'].max())\n",
    "flight_range = pd.date_range(orig_df['flight_departure_date'].min(),orig_df['flight_departure_date'].max())\n",
    "flight_df = pd.DataFrame()\n",
    "for date in flight_range:\n",
    "    if len(str(date.month)) == 1 :\n",
    "        temp_month = '0'+str(date.month)\n",
    "    else:\n",
    "        temp_month = date.month\n",
    "    if len(str(date.day)) == 1 :\n",
    "        temp_day = '0'+str(date.day)\n",
    "    else:\n",
    "        temp_day = date.day\n",
    "    with suppress(Exception): flight_df = pd.concat([flight_df, wr.s3.read_parquet(f\"s3://{bucket}/train_data/flight/pps_data/{date.year}/{temp_month}/{temp_day}/flight_df.parquet\")])\n",
    "\n",
    "flight_raw = flight_df.shape\n",
    "coupon_raw = coupon_df.shape\n",
    "booking_raw = booking_df.shape\n",
    "\n",
    "## Drop Duplicates\n",
    "coupon_df.drop_duplicates(inplace = True)\n",
    "booking_df.drop_duplicates(inplace = True)\n",
    "flight_df.drop_duplicates(inplace = True)\n",
    "\n",
    "## Merge coupon_df & booking_df & flight_df\n",
    "orig_df = pd.merge(flight_df, orig_df, on = list(set(list(flight_df)) & set(list(orig_df))), how = 'right')\n",
    "orig_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "## Create Columns\n",
    "total_df = module_v1.create_columns(orig_df, booking_df)\n",
    "total_df.dropna(axis = 0, inplace = True)\n",
    "\n",
    "## Save Data\n",
    "wr.s3.to_parquet(df = total_df,\n",
    "                 path=f\"s3://{bucket}/train_data/{datetime.datetime.today().strftime('%Y-%m-%d %H')}/train_data.parquet\")   \n",
    "\n",
    "## Data Report\n",
    "sns_arn = os.environ['snsARN']\n",
    "snsclient = boto3.client('sns')\n",
    "try:\n",
    "    message = \"\"\n",
    "    message += \"\\nLambda Data Report\" + \"\\n\\n\"\n",
    "    message += \"############################################################################\\n\"\n",
    "    message += \"# Data PreProcessing & Train Dataset Save Done \"+ \"\\n\"\n",
    "    message += \"# Coupon Shape : \" + str(coupon_raw) + \" -> Drop Duplicates Shape : \" + str(coupon_df.shape) + \"\\n\"\n",
    "    message += \"# Booking Shape : \" + str(booking_raw) + \" -> Drop Duplicates Shape : \" + str(booking_df.shape) + \"\\n\"\n",
    "    message += \"# Flight Shape : \" + str(flight_raw) + \" -> Drop Duplicates Shape : \" + str(flight_df.shape) + \"\\n\\n\"\n",
    "    message += \"# All Join Data Shape : \" + str(orig_df) + \" -> After PreProcessing 2 Shape : \" + str(total_df.shape) + \"\\n\"\n",
    "    message += \"############################################################################\\n\"\n",
    "\n",
    "    snsclient.publish(\n",
    "        TargetArn=sns_arn,\n",
    "        Subject=f'Lambda Data Report',\n",
    "        Message=message\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a15bdd4-8c8b-4063-b523-9bdddfba7396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089d2a8f-915d-4c72-a27a-522b990a790b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import datetime\n",
    "#import sagemaker\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    sm_boto3 = boto3.client(\"sagemaker\")\n",
    "    s3 = boto3.client('s3')\n",
    "    project = 'train-from-boto3'\n",
    "    source = 'source.tar.gz'\n",
    "\n",
    "    # features 정의 필요\n",
    "    # features = ['departure_hour', 'departure_minute', 'departure_time_format_min', 'sold_seats', 'total_sold_seats', 'remain_days']\n",
    "    features = ['total_sold_seats', 'startseg_CJU', 'startseg_GMP', 'remain_seats', 'departure_hour', 'departure_minute', 'departure_time_format_min', 'remain_days', \n",
    "                'departureweekday_0', 'departureweekday_1', 'departureweekday_2', 'departureweekday_3', 'departureweekday_4', 'departureweekday_5', 'departureweekday_6', \n",
    "                'issueweekday_0', 'issueweekday_1', 'issueweekday_2', 'issueweekday_3', 'issueweekday_4', 'issueweekday_5', 'issueweekday_6']\n",
    "    target = 'fare'\n",
    "    training_image = '366743142698.dkr.ecr.ap-northeast-2.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3'\n",
    "    role = 'arn:aws:iam::279545419827:role/service-role/AmazonSageMaker-ExecutionRole-20220901T101144'\n",
    "    \n",
    "    ## Load Train Data\n",
    "    obj_list = s3.list_objects(Bucket = bucket, Prefix = 'train_data')\n",
    "    obj_list = [i['Key'].split('/')[1] for i in obj_list['Contents'] if 'train_data.parquet' in i['Key']]   \n",
    "    s3uri = f's3://poc-2209-twayairport-dp/train_data/{max(obj_list)}/train_data.parquet'\n",
    "    \n",
    "    ## Start Training Job    \n",
    "    response = sm_boto3.create_training_job(TrainingJobName=\"sklearn-boto3-\" + datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"),\n",
    "                                            HyperParameters={\n",
    "                                                \"sagemaker_program\": \"script.py\",\n",
    "                                                \"features\": ' '.join(features),\n",
    "                                                \"target\": target,\n",
    "                                                \"sagemaker_submit_directory\": f\"s3://{bucket}/{project}/{source}\",\n",
    "                                            },\n",
    "                                            AlgorithmSpecification={\n",
    "                                                \"TrainingImage\": training_image,\n",
    "                                                \"TrainingInputMode\": \"File\",\n",
    "                                                \"MetricDefinitions\": [{\"Name\": \"median-AE\", \"Regex\": \"AE-at-50th-percentile: ([0-9.]+).*$\"}],\n",
    "                                            },\n",
    "                                            RoleArn=role,\n",
    "                                            InputDataConfig=[{\"ChannelName\": \"total\",\n",
    "                                                              \"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\",\n",
    "                                                                                              \"S3Uri\": s3uri,\n",
    "                                                                                              \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                                                                                             }}}],\n",
    "                                            OutputDataConfig={\"S3OutputPath\": f\"s3://{bucket}/sagemaker-sklearn-artifact/\"},\n",
    "                                            ResourceConfig={\"InstanceType\": \"ml.c5.xlarge\", \"InstanceCount\": 1, \"VolumeSizeInGB\": 10},\n",
    "                                            StoppingCondition={\"MaxRuntimeInSeconds\": 86400},\n",
    "                                            EnableNetworkIsolation=False)  \n",
    "    print(response)\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('Hello from Lambda!')\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a5031-e090-4618-80ef-14d8696bb252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b739326-78e6-4eee-989c-16db7fc9645d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc9eb5-a44b-47aa-a7e0-33f73a574c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-2:806072073708:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
